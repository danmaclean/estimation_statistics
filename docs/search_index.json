[["index.html", "Estimation Statistics Topic 1 Setting up 1.1 Prerequisites 1.2 Installing R 1.3 Installing RStudio 1.4 Installing R packages in RStudio.", " Estimation Statistics Dan MacLean 2021-02-03 Topic 1 Setting up The primary purpose of this course is to help you to understand how to use statistics that will help with your research. The course will try to explain a branch of statistics called ‘Estimation Statistics’ which are complementary to the normal sort of hypothesis test procedures and address some of the criticisms of those methods. Statistics is a computationally heavy topic, so we’ll be making use of the R statistical programming environment to do that side of the work. The rest of this chapter will help you get that set up on your own computer. 1.1 Prerequisites 1.1.1 Knowledge prerequisites There are no specific knowledge prerequisites for this book but it will help if you have read and worked through the ggplot and Intro to Stats books and are familiar with R use. 1.1.2 Software prerequisites You need to install the following stuff for this book: R RStudio Some R packages: devtools and besthr 1.2 Installing R Follow this link and install the right version for your operating system https://www.stats.bris.ac.uk/R/ 1.3 Installing RStudio Follow this link and install the right version for your operating system https://www.rstudio.com/products/rstudio/download/ 1.4 Installing R packages in RStudio. 1.4.1 Standard packages - devtools devtools is a standard R package and can be installed like any other you may have done from CRAN. Start RStudio and use the Packages tab in lower right panel. Click the install button (top left of the panel) and enter the package name, then click install as in this picture Installing Packages In the same way, you should install effectsize. 1.4.2 Development packages - itssl itssl is a new package that contains all the materials including the exercises for this handbook. As it is a development package you need to install it using devtools. In the Console tab in the lower left panel of RStudio type devtools::install_github(\"danmaclean/besthr\") You may get asked to install newer versions of packages, select 1. All for these questions. "],["motivation.html", "Topic 2 Motivation 2.1 Variability in measurements 2.2 Summarising your data can lead to wrong conclusions 2.3 p - one value to fool them all? 2.4 Estimation Statistics", " Topic 2 Motivation 2.1 Variability in measurements Variability in measurements is a thing that happens as a natural consequence of working with complex systems that are affected by many variables in stochastic ways. Biological systems are some of the most variable we know. The variability in our experiments could be a function of the behaviour of the system yet it is common practice to hide that variability when we start to analyse our data by using summary plots like box-plots. Ultimately, that’s bad news for our science, because the variability could be telling us something. 2.2 Summarising your data can lead to wrong conclusions We all know that when you create a bar chart and put some error bars on it, you’re really only representing two numbers, usually a mean and standard deviation. People create bar plots instinctively, and in doing so can miss important stuff. Look at this figure: source: Weissgerber et al The bar chart in panel A is one that came out of all those sets of numbers in the other panels. But it really hides some important stuff, like the fact the numbers are clearly separating into two groups in panel D, or that the two samples have different sizes in panel E. Worse than any of these is that the significant difference in the t-test is coming from just one point in panel C. From this data set you might be tempted to conclude that there is a significant difference in the two samples and if you relied on the bar chart as a visualisation then you’d never suspect there was something funny. Some enthusiastic young science communicators have even started a Kickstarter to lobby journals to stop using, in particular, bar charts! These people, calling themselves Bar Barplots, have a nice video on one of the main problems with bar charts. Have a look at this page on Kickstarter . Kickstarter - Barbarplots, especially this video Kickstarter - Barbarplots video. Ignoring your data visualisation and just making bar plots could be an error! It’s important that you spend a little time getting to know, and presenting your data as clearly and thoroughly as possible. 2.3 p - one value to fool them all? But why would you care about this, in the end a p-value won’t a p-value help you see real differences and make this all easy? Sadly, that isn’t true. Let’s do an experiment to test that. 2.3.1 Ten Thousand Random Numbers Below is a set of figures that show different views of the same set of data. Every frame of the 100 frames shows a different sampling from the same pool of 10,000 random normally distributed numbers. Random Sample Plots Step-by-step, here’s how these figures are made. Generate a pool of 10,000 random numbers (mean 5, sd 1) From that, select 10 and call it sample 1. Select another 10, call it sample 2. Draw plots comparing each sample Do an independent t-test on the sample 1 and sample 2 to test for significant differences in means. The figures are plotted with a red border if p comes up less than 0.05. The thing is, the samples are from the same background pool, so intuitively you might suspect that none should be different from the others. The reason that some of them do is because a p value only states that the difference observed occurs by chance in p of all events, so for 100, we’d expect 5 to be marked out by chance. In this run of the experiment we get three. Here they are: Look at the different plots for each. It is observable that for all these the barplots look very convincingly different. But in the context with the other plots its clear that they aren’t showing the whole of the story (or in fact much of it). The boxplots (top left) do a good job of showing the range and the violin and density plots (bottom row) do a good job of showing the shape. It is only really the point plot (first column, middle row) that reveals the positions of the data points and shows that the conclusion of the p value is likely skewed by one or two points in each sample. Concluding differences on this basis is really unsafe. Hence, the conclusion from this is that a range of visualisations is necessary to allow us to have confidence in our p values and understand the shapes of our data. Drawing box plots and sticking to p religiously is going to make us wrong more than we’d like! 2.4 Estimation Statistics So what can we do about this apparent problem? One strategy is to take a much less hypothesis test-centric approach to statistical analysis and apply other ways of thinking about the problems, instead we can use Estimation Statistics to look at the parameters of our data and get an idea of magnitude of an effect or difference and an associated confidence interval that estimates how believable that estimate is. Hey, isn’t all this rationale in the ggplot book? Umm, it is a bit, yeah! Graphical methods and Estimation Statistics are closely related because plots can show the parameters very well In this book we’ll take a look at some of the main features of Estimation Statistics with the intention that they’ll be useful in your future research. The same rule about taking a \\(p\\)-value as your only determinant of whether a statistical claim is important or significant applies to the Estimation Statistics we’ll learn here. At the end of this book we will examine the integration of the different estimate statistics covered. "],["r-fundamentals.html", "Topic 3 R Fundamentals 3.1 About this chapter 3.2 Working with R 3.3 Variables 3.4 Dataframes 3.5 Packages 3.6 Using R Help", " Topic 3 R Fundamentals 3.1 About this chapter Questions: How do I use R? Objectives: Become familiar with R syntax Understand the concepts of objects and assignment Get exposed to a few functions Keypoints: R’s capabilities are provided by functions R users call functions and get results 3.2 Working with R In this workshop we’ll use R in the extremely useful RStudio software. For the most part we’ll work interactively, meaning we’ll type stuff straight into the R console in RStudio (Usually this is a window on the left or lower left) and get our results there too (usually in the console or in a window on the right). Panels like the ones below mimic the interaction with R and first show the thing to type into R, and below the calculated result from R. Let’s look at how R works by using it for it’s most basic job - as a calculator: 3 + 5 ## [1] 8 12 * 2 ## [1] 24 1 / 3 ## [1] 0.3333333 12 * 2 ## [1] 24 Fairly straightforward, we type in the expression and we get a result. That’s how this whole book will work, you type the stuff in, and get answers out. It’ll be easiest to learn if you go ahead and copy the examples one by one. Try to resist the urge to use copy and paste. Typing longhand really encourages you to look at what you’re entering. As far as the R output itself goes, it’s really straightforward - its just the answer with a [1] stuck on the front. This [1] tells us how many items through the output we are. Often R will return long lists of numbers and it can be helpful to have this extra information. 3.3 Variables We can save the output of operations for later use by giving it a name using the assignment symbol &lt;-. Read this symbol as ‘gets’, so x &lt;- 5 reads as ‘x gets 5’. These names are called variables, because the value they are associated with can change. Let’s give five a name, x then refer to the value 5 by it’s name. We can then use the name in place of the value. In the jargon of computing we say we are assigning a value to a variable. x &lt;- 5 x ## [1] 5 x * 2 ## [1] 10 y &lt;- 3 x * y ## [1] 15 This is of course of limited value with just numbers but is of great value when we have large datasets, as the whole thing can be referred to by the variable. 3.3.1 Using objects and functions At the top level, R is a simple language with two types of thing: functions and objects. As a user you will use functions to do stuff, and get back objects as an answer. Functions are easy to spot, they are a name followed by a pair of brackets. A function like mean() is the function for calculating a mean. The options (or arguments) for the function go inside the brackets: sqrt(16) ## [1] 4 Often the result from a function will be more complicated than a simple number object, often it will be a vector (simple list), like from the rnorm() function that returns lists of random numbers rnorm(100) ## [1] -1.233362585 -0.292018945 -1.220396507 2.768132613 -0.777617798 ## [6] 0.483401660 0.679406350 0.029285960 -0.113899463 1.919474392 ## [11] 1.337035054 0.459189175 0.315171204 0.991973779 0.058471097 ## [16] -0.343008304 -0.087979977 1.582757060 0.661542290 1.570250059 ## [21] 0.158649927 0.100960080 0.402380290 0.108313240 0.655167715 ## [26] 0.690074970 -1.381277823 1.204807266 1.099055561 -1.332565687 ## [31] 0.428895762 0.674837947 1.373063409 0.606458404 -1.233314334 ## [36] -2.161014920 -0.389889304 0.108535269 -0.437790050 -0.501394123 ## [41] -1.992905573 -0.128252435 -1.427136913 -0.294718801 0.847395269 ## [46] 0.090265291 1.293733201 0.232839684 -1.207222302 -1.242643530 ## [51] -0.658166422 -0.460251461 -0.333461287 -0.806898897 1.114252779 ## [56] -0.754800366 1.559928663 -0.102500642 -0.196004835 0.437862648 ## [61] 1.432694843 -0.551363846 -0.137071197 0.449535818 1.191979266 ## [66] -1.317756583 -0.688775860 1.124191726 -1.658902145 0.236425495 ## [71] 0.040168262 0.942878268 -0.162604725 -0.733365289 -0.711432531 ## [76] 1.098441931 -0.282164520 -1.845571692 0.706783324 1.206335699 ## [81] -0.441429655 0.642154135 0.738718806 -0.005209885 -0.648360743 ## [86] -0.114104539 0.747222554 0.437337408 -0.679845528 0.741779993 ## [91] 0.500883080 2.146241325 2.019965611 0.818034480 -0.521130031 ## [96] 0.302078075 -0.374030281 -1.816398264 -0.251551351 0.751760673 We can combine objects, variables and functions to do more complex stuff in R, here’s how we get the mean of 100 random numbers. numbers &lt;- rnorm(100) mean(numbers) ## [1] 0.1167265 Here we created a vector object with rnorm(100) and assigned it to the variable numbers. We than used the mean() function, passing it the variable numbers. The mean() function returned the mean of the hundred random numbers. 3.4 Dataframes One of the more common objects that R uses is a dataframe. The dataframe is a rectangular table-like object that contains data, think of it like a spreadsheet tab. Like the spreadsheet, the dataframe has rows and columns, the columns have names and the different columns can have different types of data in. Here’s a little one ## names age score ## 1 Guido 24 46.14678 ## 2 Marty 45 72.65534 ## 3 Alan 11 59.74223 Usually we get a dataframe by loading in data from an external source or as a result from functions, occasionally we’ll want to hand make one, which can be done with various functions, data.frame being the most common. data.frame( names = c(&quot;Guido&quot;, &quot;Marty&quot;, &quot;Alan&quot;), age = c(24,45,11), score = runif(3) * 100 ) 3.5 Packages Many of the tools we use in will come in R packages, little nuggets of code that group related functions together. Installing new packages can be done using the Packages pane of RStudio or the install.packages() function. When we wish to use that code we use the library() function library(somepackage) 3.6 Using R Help R provides a command, called ? that will display the documentation for functions. For example ?mean will display the help for the mean() function. ?mean As in all programming languages the internal documentation in R is written with some assumption that the reader is familiar with the language. This can be a pain when you are starting out as the help will seem a bit obscure at times. Don’t worry about this, usually the Examples section will give you a good idea of how to use the function and as your experience grows then the more things will make more sense. * R is an excellent and powerful statistical computing environment Complete the interactive tutorial online https://danmaclean.shinyapps.io/r-start "],["effect-size.html", "Topic 4 Effect Size 4.1 Difference in sample means 4.2 Explained variation 4.3 Using the effect size 4.4 The assumptions of effect sizes", " Topic 4 Effect Size The first of the three central ideas in Estimation Statistics is the concept of the effect size. This is an essential component in assessing the strength of a relationship between variables and a critical tool in working out whether a statistical claim (like significance) is valid or not You will already be familiar with the most common effect sizes, these are common metrics like the sample mean difference, the regression coefficient (\\(r\\)) from a regression analysis or the likelihood of an event. 4.1 Difference in sample means The most commonly seen effect size is the difference between sample means. Despite the apparent simplicity and to an extent, obviousness, of the difference in the means of two groups this parameter is a very useful tool in determining whether or not a claim of difference or significance is valid or not. One advantage and disadvantage of using the difference in sample means is that to have a good intuition about the claim of significance we must have some prior experience about whether the observed effect size is a big or small one. Large effect sizes are more significant, all other things being equal, but the absolute effect size is highly dependent on the specifics of the domain we are working in. For example 10g weight difference between two groups of adult humans is not going to be seen as significant, however 10g in difference in groups of adult domestic mice is quite large. If we’re going to be formal about it, then we can write the absolute effect size as follows: Where \\(n_i\\) is the sample size for group \\(x_i\\) \\[\\frac{\\sum{x_1}}{n_1} - \\frac{\\sum{x_2}}{n_2}\\] Which looks more complicated than it is. The formula just describes the mean of the second group subtracted from the mean of the first group. We can simplify each term thus \\[\\bar{x_i} = \\frac{\\sum{x_i}}{n_i}\\] Such that the formula for effect size becomes \\[\\bar{x_1} - \\bar{x_2}\\] And \\(\\bar{x}\\) is a pretty standard name for a sample mean in the statistical literature. 4.1.1 Standardised Effect Sizes It would be extremely difficult to say whether even a 100g difference in human weight is as of as great significance as 10g in mice. Given that the real world meaningfulness of the mean sample difference is dependent upon the context we often need a mechanism through which we can compare effect sizes. Standardised effect sizes can help with this, and they work by taking into account the pooled standard deviation of the two samples, expressing the mean difference in terms of the variation in the numbers that make up the mean. If \\(s\\) is the pooled standard deviation then our standardised effect size \\(d\\) is \\[d =\\frac{\\bar{x_1} - \\bar{x_2}}{s}\\] Computing this for our mouse and human data would enable us to make reasonable comparisons between the significance of the effect size in the different experiments. The standardised effect size was introduced by Jacob Cohen and the number is known as Cohen’s \\(d\\). Cohen also suggested descriptions of the different values of \\(d\\) and the effect size: Effect.Size d Very Small 0.01 Small 0.20 Medium 0.50 Large 0.80 Very Large 1.20 Huge 2.00 So if we get a \\(d\\) of 0.7, we have a medium to large effect size. Computing \\(s\\) is done according to a formula called the pooled standard deviation for two independent samples. If you don’t look at lot’s of formula it looks a little scary at first glance so I haven’t included it here, I mention it just to point out that it isn’t the same as adding up the two individual sample standard deviations. 4.1.2 Calculating Mean Difference Effect Sizes in Practice Thankfully there are functions in R that we can use to calculate each of the quantities we have mentioned straight from the data we collect, so we don’t need to know all the formulae off the top of our heads. We can use the effectsize library for this. First though, let’s generate some samples of data from a random normal distribution with mouse sized and human sized means (measuring their weight in grams), but a standard deviation that is the same proportion of the mean in each Domestic mice are about 20 g in mass. set.seed(123) # ensure random numbers are identical every time library(effectsize) x_mouse &lt;- rnorm(10, mean = 20, sd = 20 / 3 ) y_mouse &lt;- rnorm(10, mean = 10, sd = 10 / 3 ) cohens_d(x_mouse, y_mouse) ## Cohen&#39;s d | 95% CI ## ------------------------ ## 1.91 | [0.82, 2.97] ## ## - Estimated using pooled SD. So we get an effect size which is pretty large! Now let’s try the human data, humans are about 65 kg in mass (depending on where you measure!) x_human &lt;- rnorm(10, mean = 65000, sd = 65000 / 3 ) y_human &lt;- rnorm(10, mean = 32500, sd = 32500 / 3 ) cohens_d(x_human, y_human) ## Cohen&#39;s d | 95% CI ## ------------------------ ## 1.34 | [0.34, 2.30] ## ## - Estimated using pooled SD. The human effect size is of similar magnitude to that of the mouse, that is ‘large’, from intuition on both sets of measurements we can see that the halving of mass is a big effect, so it matches up. Let’s try the human measurements at a mouse size change x_human &lt;- rnorm(10, mean = 65000, sd = 65000 / 3 ) y_human &lt;- rnorm(10, mean = 64990, sd = 64990 / 3 ) cohens_d(x_human, y_human) ## Cohen&#39;s d | 95% CI ## ------------------------- ## -0.24 | [-1.11, 0.65] ## ## - Estimated using pooled SD. As expected the effect size reduces to small. 4.2 Explained variation Another common type of effect size is that computed for correlation style data (like that we see in linear regression models), so when we have a continuous \\(x\\) (explanatory variable) and a response . These effect sizes are based on the amount of the variance that is captured by the model (like a linear model). In other terms we’ve thought about explained variation as the fit to the model. We can visualise this in a scatter plot. The greater the explained variation, the better the fit to the line. It is clear to see that the line (and therefore model) fits the data in the left panel much better than the line (model) fits the data in the right panel. The effect size of the better fit model is going to be larger. 4.2.1 Pearson’s \\(r\\) Pearson’s \\(r\\) value is one we are likely familiar with from correlation analysis and is a simple effect size we can use with continuous data. It runs from -1 to 1 with values around 0 indicating a smaller effect size and values at -1 or 1 indicating larger effect sizes. The sign (+/-) of \\(r\\) indicates only the nature of the correlation, not the effect sizes, so -0.3 and + 0.3 are equivalently sized negative and positive correlations. The value of this effect size is interpreted differently from that of Cohen’s \\(d\\), as the values can only run from -1 to 1, here’s a brief table of categories Effect.Size r Small 0.1 Medium 0.3 Large 0.5 These values vary from domain to domain, in some domains we would expect a much stronger correlation and correspondingly larger values of \\(r\\) to give the same description of an effect size. Consider correlations between performances of machines like car engines, it’d be very surprising if they didn’t correlate in the very high 0.9s, whereas correlations of biological measurements would be good at a much lower level. The values given in the table above were stated by Cohen (again) for the Social Sciences. You’ll need to make conclusions judiciously and with an informed mind (and again in conjunction with other measures) for the domain you are working in. As it is fundamental, calculating Pearson’s \\(r\\) is easy in R. Let’s use the data from the plots above to run through it. The left plot data is in a dataframe called df1, the right plot data is in a dataframe called df2. The function we need is cor(), which is part of base R. cor(df1$x, df1$y) ## [1] 0.9716088 Which gives a high correlation indeed. For the less well fitting data we see this cor(df2$x, df2$y) ## [1] 0.1213444 a much lower Pearson’s \\(r\\). 4.2.2 \\(r^2\\) A related effect size is \\(r^2\\) which is literally \\(r \\times r\\). It measures the proportion of variance shared between the two variables under examination, so can be interpreted as the amount of variance explained. This one naturally runs between values of 0 and 1 so loses the information about direction of correlation. 4.3 Using the effect size At the beginning of this course we cautioned about using \\(p\\)-values as the sole measure to decide whether a claim about differences is significant or important. The same caution applies to the effect size, whether you use Cohen’s \\(d\\), Pearson’s \\(r\\) or some other measure. There’s no cut-off that always makes sense, so never use it on it’s own. Use your expertise and other measures we’ll see later (and including but not limited to hypothesis tests and \\(p\\)-values) to make data informed interpretations about your results and never rely on arbitrary cut-offs. At the end of this book we will examine the integration of the different estimate statistics covered. 4.4 The assumptions of effect sizes As we’ve discovered before, statisticians make assumptions about data when discovering statistics. The standard assumption is that the data and the variance are Normally Distributed (so follow the Normal Distribution). The effect sizes we’be discussed here make that assumption in the calculations too. Practically, it means that the further you get away from a ‘Normal’ situation the less use the named effect size formula will be. There are other effect sizes for non-Normal data, notably the Spearman’s Rank \\(r\\) and Kendall’s \\(\\tau\\) for ranked and categorical correlations. Sometimes the raw mean difference is the only practical measure of effect size. Although it is important that you are aware whether your data are close to Normal or not, the problem isn’t always drastic and in combination with other measures we can reduce misuse of single statistics and we will look at ways of dealing with arbitrary distributions with Estimation Statistics. "],["confidence-intervals.html", "Topic 5 Confidence Intervals 5.1 Confidence Intervals 5.2 Confidence intervals on Normally distributed data 5.3 Using a CI 5.4 Bootstrap Estimation of Confidence Intervals 5.5 Plotting the bootstrap distribution", " Topic 5 Confidence Intervals 5.1 Confidence Intervals A confidence interval (CI) is a range estimate of a statistical parameter (a number or measure of some sort) that is computed from the data. It gives a range that the parameter falls in with as certain confidence (the confidence value), so a 95% Confidence Interval of the sample mean tells us the range in which we believe the sample mean will lie. We often see them rendered like this But wait! Isn’t the first one error bars? Isn’t the second one percentiles of a distribution? That’s right, in fact the visual glyph used for confidence intervals and error bars or confidence intervals and percentiles are the same. This is because they are each a kind of numeric range and we can show them on figures in the same way. We must be careful not confuse them because they seldom show the same thing. To be explicit again confidence intervals show the likely range that a parameter (like a mean) falls in. Error bars usually show a sample mean plus or minus a fixed number like the standard deviation, but this doesn’t necessarily say anything about the confidence we have that the mean falls in that range. Similarly, percentiles show how much of the observed data fall in a range, e.g the 10th percentile shows where 10% of the data fall, but it again doesn’t necessarily say anything about the confidence we have about where a particular parameter falls. 5.2 Confidence intervals on Normally distributed data A common use of confidence intervals is in conjunction with the Normal Distribution as a Null Model. In which we mean we assume that the variability of a measurement, e.g mouse mass is Normally distributed. As the Normal distribution is well understood we can use its characteristics to get a CI for the mean of a sample. Let’s invent some mouse weights and put them in a vector x. mouse_wts ## [1] 19.57635 29.59349 23.89572 17.75500 11.47902 29.90031 26.04292 30.18206 ## [9] 30.28894 8.09662 We can use the qnorm() function to get the limits of a given CI on a standardised Normal Distribution. We provide the CI size (here we’ll use 95%) but because a 95% confidence interval has a total of 5% that must fall outside of each end of the interval, we use half of 5% (2.5%) in the function (and we provide it as a probability to this function so it’s 0.975). We can then plug this into a standard formula to get the half width of the CI using the mean and standard deviation of the sample. Finally, we can add and subtract this value from the sample mean to get the lower and upper bounds of the CI we want. This isn’t quite as complicated as it reads, here’s the code x_bar &lt;- mean(mouse_wts) s &lt;- sd(mouse_wts) n &lt;- length(mouse_wts) half_width &lt;- qnorm(0.975) * s / sqrt(n) left_bound &lt;- x_bar - half_width right_bound &lt;- x_bar + half_width left_bound ## [1] 17.625 right_bound ## [1] 27.73708 So we calculated that the 95% CI of the mean of mouse weights runs from 17.6250015 to 27.7370849. The derivation of this formula is generally available in every statistics text book, so I won’t repeat it here, I’ll leave it to the interested reader to follow up if they wish. 5.3 Using a CI Now that we have a CI constructed we can use it. One fact about CIs calculated on a sample is that the true mean of the population (E.G the weights of all mice in the world, not just the ones we sampled) falls within an \\(x\\)% CI \\(x\\)% of the time. We can use this fact to estimate whether another measurement like the mean of another sample falls within a CI. If it doesn’t then we can say the mean of the second sample only occurs in samples from the population (100 - \\(x\\)%) of the time. So if another sample mean falls outside of our 95% CI we can say that two samples with means this far apart only occur by chance less than 5% of the time. This is actually analogous to how a hypothesis test works, particularly \\(t\\)-tests. So the interpretation of a difference in populations is analogous. Here’s a graphical representation of that on our mouse data The dotted blue line represents a measurement of a single mouse, the green areas in the density plot are the area’s outside the 95% CI we computed. So the specific measurement looks like it wouldn’t come from our population very often. 5.3.1 Confidence Intervals in practice Of course, looking at the density curve we see that it is not very convincing as the curve of a Normal distribution and the areas outside the CI seem wider than we might have expected from textbook renderings of a Normal distribution. The numbers for mouse weights were drawn randomly from a Normal distribution and the off pattern is due to the small sample size, we only used 10 mouse weight measurement. As a result the estimation of the curve is quite lumpy. Such a phenomenon not only highlights the need for sensibly sized samples but also indicates the importance of careful examination of the data on which we are creating confidence intervals. To repeat our familiar refrain you can’t use a single measure on it’s own. Other measures should be integrated and confidence intervals are best interpreted along side full representations of the data. Here is the same plot with a histogram that shows more finely the actual data We can see from the histogram that the data are pretty gappy. Most bins have just one measurement in them (see the right hand scale), so the sparse sampling makes our curve density plot of the data less Normal looking. Putting the histogram data on with the confidence interval gives a much clearer view of the data and improves the interpretability of the data. 5.4 Bootstrap Estimation of Confidence Intervals What can we do about estimating a confidence interval of a parameter like a sample mean when we don’t know or can’t reasonably assume that the data are drawn from a Normal distribution? We can’t apply the formula we saw above as that is tied to assumptions of Normality, but we can use brute force computer power to create a bootstrap confidence intervals. Bootstrapping is a resampling technique that builds a bigger data set out of a smaller one and hopes to work out what the spread of data would be. Crucially, it doesn’t rely on assumptions and can be used on any sort of data. The logic of bootstrapping goes as follows: We have a set of numbers in a sample that came from a population, so we know these appear in the population If we take lots of samples from our sample, and each time recalculate the parameter we are trying to estimate we can get a distribution of parameter estimates. The distribution of parameter estimates tells us how likely a given parameter estimate is based on the data In summary it means we can create a confidence interval for our parameter from the data we have. This ‘magic’ step is where bootstrapping gets its name from - the phrase “pulling yourself up by your bootstraps” which basically means to create something for yourself from nothing - literally lift yourself up by grabbing hold and pulling on your boots Let’s work through this step-by-step to make it clearer. Starting with the mouse_wts data, lets make a random sample of the values of mouse_wts of equal length to mouse_wts (IE 10) and get the mean of that sample_1 &lt;- sample(mouse_wts, 10, replace = TRUE) x_bar_sample_1 &lt;- mean(sample_1) Note that we sample ‘with replacement’ (we effectively put each number back to be selected again if needed) if we didn’t we’d end up with just the same set of numbers as in the original mouse_wts which is what we don’t want - we want a sample of the original numbers. So conceivably we could get the same number from mouse_wts all the time. This is actually a desirable feature. Compare mouse_wts and sample_1 mouse_wts ## [1] 19.57635 29.59349 23.89572 17.75500 11.47902 29.90031 26.04292 30.18206 ## [9] 30.28894 8.09662 sample_1 ## [1] 19.57635 29.59349 26.04292 30.18206 30.28894 11.47902 26.04292 29.90031 ## [9] 11.47902 23.89572 Some of the same numbers do indeed repeat and some numbers do not appear. The mean of the sample_1 was 23.848075, this is different to the mean of mouse_wts which was 22.6810432 We get a different set of numbers and a different mean each time we iterate this process. If we iterate this many times (like thousands, usually) we get a distribution of the parameter of interest, the mean. The bootstrap function in the resample package takes care of this for us. We give it the data we want to sample from, the name of the statistic we want to compute (here mean) and the number of replicates to do. library(resample) bstrap_estimates &lt;- bootstrap(mouse_wts, statistic=mean, R=10000) We can get the values of the CI with the CI.percentile() function, passing in the lower and upper bound for the CI with the probs argument. CI.percentile(bstrap_estimates, probs=c(0.025, 0.975)) ## 2.5% 97.5% ## mean 16.6212 27.9609 So, this is our 95% CI for the mean of mouse_wts. How does it compare with our earlier computed values? It’s actually really close. left_bound ## [1] 17.625 right_bound ## [1] 27.73708 5.5 Plotting the bootstrap distribution We would often want to view the bootstrap distribution to understand the CI of our mean. The function returns a complex object and the actual computed values are in the replicates slot which we can get with the $ syntax and convert to a data frame and use in a typical plot. results &lt;- as.data.frame(bstrap_estimates$replicates) ggplot(results) + aes(mean, y=..density..) + geom_histogram(colour=&quot;steelblue&quot;, alpha=0.5) + geom_density() + theme_minimal() The histogram shows that most of the bootstrap resample means were around 22, further the distribution is smooth and approaches the Normal distribution, reflecting the underlying data from which the original mouse_wts values were drawn. The sample size of 1000 bootstrap replicates contributes to this and it is clear that the bootstrap CI is reliable as to the position of the mean of the data from which mouse_wts was sampled. Bootstrap sampling can be a great way to get a confidence interval, it is particularly useful when we have smallish sample sizes and/or don’t know the background distribution. "],["bootstrap-sampling.html", "Topic 6 Bootstrap Sampling", " Topic 6 Bootstrap Sampling "],["references.html", "References", " References "]]
